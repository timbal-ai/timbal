---
title: "Evals"
description: "Automated Testing for Timbal Agents"
---

AI outputs are non-deterministic â€” the same input can yield different results. 

**Evals** in Timbal help you measure, test, and improve your agents and flows with automated, LLM-powered checks.

## What Are Evals in Timbal?

Evals are automated tests that assess your agent's outputs and tool usage. They use LLMs to compare your agent's process (the tools it used and how) and its result (the answer it gave) to the expected process and result. Evals can be run locally, in the cloud, or as part of your CI/CD pipeline.

### Types of Evals

- **Input Evals**: Did the agent receive the correct input, with proper formatting and expected values?
- **Output Evals**: Did the agent produce the correct answer, with proper content and formatting?
- **Steps Evals**: Did the agent use the right tools, in the right order, with the right inputs?

## How Evals Work

### Core Concepts

#### Test Suite

A YAML file containing one or more test cases. A test suite file can contain multiple tests that validate different aspects of your agent's behavior. Timbal automatically discovers all files matching the pattern `eval*.yaml` in your test directory.

#### Test

A single test case with multiple turns, defined within a test suite (YAML file). Each test focuses on a specific scenario or functionality of your agent and has a unique name. Multiple tests can be defined in the same YAML file, and tests can be run individually or as part of a suite.

#### Turn

One interaction between user and agent. A test can contain multiple turns to validate multi-step conversations. **Only the last turn is evaluated** - previous turns establish conversation context by setting inputs and outputs (no validation occurs). The last turn consists of:
- **Input**: What the user says or asks (text and optionally files) - **required**
- **Output**: What the agent should respond (validated against expected content) - *optional*
- **Steps**: Tools the agent should use (validated against expected tool calls) - *optional*

#### Validators

Programmatic checks that compare actual vs expected behavior. Validators use both exact matching and LLM-powered semantic evaluation to assess correctness.

## Writing Tests

Evals are defined in YAML files. A test file contains one or more tests, each with one or more turns. Let's break down each part:

### Basic Test Structure

```yaml
- name: test_name
  description: Optional test description
  turns:
    - input: "User message here"
      output: "Expected agent response"
```

### Input

The `input` defines what the user says to the agent. The user's text message goes in `prompt`:

```yaml
input:
  prompt: "What is 2 + 2?"
```

**Shorthand**: `input: "What is 2 + 2?"` is equivalent to `input: { prompt: "What is 2 + 2?" }`

**Files**: Include files using the `"@"` prefix:
```yaml
input:
  prompt:
    - "Analyze this file"
    - "@./document.pdf"
```

**Additional keys**: You can add other keys that are passed to the agent as default values:
```yaml
input:
  prompt: "Hello"
  points: "10"  # This value is passed to the agent
```

**Validators**: You can validate input values. There are two types:
- **Top-level**: Only `usage` can be top-level (validates turn metrics):
  ```yaml
  input:
    prompt: "Hello"
    validators:
      usage:
        "gpt-4.1-mini:input_text_tokens":
          max: 5000
  ```
- **Per-key**: Other validators go inside specific keys:
  ```yaml
  input:
    prompt: "Hello"
    points:
      validators:
        equals: "10"  # Validates that points equals "10"
  ```

### Output

The `output` defines what the agent should respond. The response content goes in `content`:

```yaml
output:
  content: "Expected response"
```

**Shorthand**: `output: "Expected response"` is equivalent to `output: { content: "Expected response" }`

**Default value**: If no validators, used as fixed record for conversation history:
```yaml
output:
  content: "You have 10 points"  # Goes to memory, not validated
```

**Validators**: There are two types:
- **Top-level**: `time` and `usage` can be top-level (validate turn metrics):
  ```yaml
  output:
    content: "Response"
    validators:
      time:
        max: 10.0
      usage:
        "gpt-4.1-mini:output_text_tokens":
          max: 500
  ```
- **Per-key**: Other validators go inside `content.validators`:
  ```yaml
  output:
    content:
      validators:
        contains: ["expected"]
        regex: "^Success.*"
  ```

### Steps

The `steps` define which tools the agent should use:

```yaml
steps:
  validators:
    contains:
      - name: tool_name
```

**Step input validation**: You can validate tool input parameters. There are two ways:

1. **Direct values** (exact match by default):
```yaml
steps:
  validators:
    contains:
      - name: tool_name
        input:
          parameter: "value"
```

2. **With validators** for each input key:
```yaml
steps:
  validators:
    contains:
      - name: tool_name
        input:
          parameter:
            validators:
              equals: "exact_value"
              contains: "substring"
```

**Step-specific validators**: `time` and `usage` can be nested within steps:
```yaml
steps:
  validators:
    contains:
      - name: tool_name
        validators:
          time:
            max: 0.5
          usage:
            "gpt-4.1-mini:web_search_requests":
              equals: 1
```

## Validators

Timbal provides several types of validators organized by where they are used:

### Input Validators

#### Input Top-Level Validators

Validators placed at `input.validators` level (validates turn metrics):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `usage` | Validates resource consumption at turn level. Format: `"model_name:usage_type"` (e.g., `"gpt-4.1-mini:input_text_tokens"`) | `input: { validators: { usage: { "gpt-4.1-mini:input_text_tokens": { max: 1000 } } } }` |

#### Input Per-Key Validators

Validators nested under specific input keys (validates key values):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `contains` | Checks if value includes specified substrings | `points: { validators: { contains: ["10"] } }` |
| `not_contains` | Checks if value does NOT include specified substrings | `points: { validators: { not_contains: ["error"] } }` |
| `equals` | Checks if value exactly matches the specified text | `points: { validators: { equals: "10" } }` |
| `regex` | Checks if value matches a regular expression pattern | `points: { validators: { regex: "^\\d+$" } }` |
| `semantic` | Uses LLM to validate semantic correctness against reference | `points: { validators: { semantic: "Should be a number" } }` |
| `contains_any` | Checks if value includes any of the specified substrings | `points: { validators: { contains_any: ["10", "20", "30"] } }` |

### Output Validators

#### Output Top-Level Validators

Validators placed at `output.validators` level (validates turn metrics):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `time` | Validates execution time for the turn | `output: { validators: { time: { max: 10.0, min: 0.5 } } }` |
| `usage` | Validates resource consumption at turn level. Format: `"model_name:usage_type"` (e.g., `"gpt-4.1-mini:output_text_tokens"`) | `output: { validators: { usage: { "gpt-4.1-mini:output_text_tokens": { max: 500 } } } }` |

#### Output Per-Key Validators

Validators nested under `content.validators` (validates output content):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `contains` | Checks if output includes specified substrings | `content: { validators: { contains: ["hello", "world"] } }` |
| `not_contains` | Checks if output does NOT include specified substrings | `content: { validators: { not_contains: ["error", "failed"] } }` |
| `equals` | Checks if output exactly matches the specified text | `content: { validators: { equals: "Hello, world!" } }` |
| `regex` | Checks if output matches a regular expression pattern | `content: { validators: { regex: "^Success: .+" } }` |
| `semantic` | Uses LLM to validate semantic correctness against reference | `content: { validators: { semantic: "Should greet user politely" } }` |
| `contains_any` | Checks if output includes any of the specified substrings | `content: { validators: { contains_any: ["hello", "hi", "greetings"] } }` |

### Steps Validators

#### Steps Top-Level Validators

Validators placed at `steps.validators` level (validates all steps):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `contains` | Checks if steps include specified tool calls with inputs (order is NOT validated) | `steps: { validators: { contains: [{"name": "search", "input": {"query": "test"}}] } }` |
| `contains_ordered` | Checks if steps include specified tool calls in the specified order (allows additional steps between them) | `steps: { validators: { contains_ordered: [{"name": "search"}, {"name": "calculate"}] } }` |
| `equals` | Checks if steps exactly match the specified tool calls in the exact order, with no additional steps | `steps: { validators: { equals: [{"name": "search"}, {"name": "calculate"}] } }` |
| `not_contains` | Checks if steps do NOT include specified tool calls | `steps: { validators: { not_contains: [{"name": "delete_file"}] } }` |
| `contains_any` | Checks if steps include any of the specified tool calls | `steps: { validators: { contains_any: [{"name": "search"}, {"name": "find"}] } }` |
| `semantic` | Uses LLM to validate tool usage against expected behavior (may consider logical order) | `steps: { validators: { semantic: "Should search before providing answer" } }` |

**Note**: 
- The `contains` validator does NOT validate the order of steps. It only verifies that each expected step exists somewhere in the actual steps, regardless of their position.
- The `contains_ordered` validator validates that steps appear in the specified relative order, but allows additional steps between them.
- The `equals` validator validates the exact order and ensures no additional steps are present. Steps must match exactly in the same order.

#### Steps Step-Specific Validators

Validators nested within `contains` items (validates individual steps):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `time` | Validates execution time for a specific step | `contains: [{"name": "tool", "validators": {"time": {"max": 0.5}}}]` |
| `usage` | Validates resource consumption for a specific step. Format: `"model_name:usage_type"` | `contains: [{"name": "web_search", "validators": {"usage": {"gpt-4.1-mini:web_search_requests": {"equals": 1}}}}]` |

**Note**: `time` validator is NOT supported at `steps.validators` level since tools can execute in parallel. Use step-specific validators instead.

#### Steps Step Input Validators

Validators for step input parameters (nested under `input.parameter.validators`):

| Validator | Description | Example Usage |
|-----------|-------------|---------------|
| `equals` | Exact string match | `input: { parameter: { validators: { equals: "5" } } }` |
| `contains` | Value must contain substring | `input: { parameter: { validators: { contains: "substring" } } }` |
| `not_contains` | Value must NOT contain substring | `input: { parameter: { validators: { not_contains: "error" } } }` |
| `regex` | Value must match regular expression pattern | `input: { parameter: { validators: { regex: "^[A-Z]" } } }` |

**Note**: If no validator is specified, the value is checked for exact match (string comparison).

## Running Evals

### Command Line Interface

```bash
# Run all tests in a directory
python -m timbal.eval --fqn path/to/agent.py::agent_name --tests path/to/tests/

# Run a specific test file
python -m timbal.eval --fqn path/to/agent.py::agent_name --tests path/to/tests/eval_search.yaml

# Run a specific test by name
python -m timbal.eval --fqn path/to/agent.py::agent_name --tests path/to/tests/eval_search.yaml::test_basic_search
```

### Command Options

- **`--fqn`**: Fully qualified name of your agent (format: `file.py::agent_name`)
- **`--tests`**: Path to test file, directory, or specific test (format: `path/file.yaml::test_name`)

## Examples

### Example 1: Product Search with Complete Validation

```yaml
- name: eval_reference_product
  description: Test product search by reference code
  turns:
    - input:
        prompt: "tell me the measurements of H214E/1"
        validators:
          usage:
            "gpt-4.1-mini:input_text_tokens":
              max: 5000
              min: 1000
      steps: 
        validators:
          contains:
            - name: search_by_reference
              input:
                reference_code: H214E/1
          semantic: "Should search for product using the exact reference code H214E/1"
      output: 
        content:
          validators: 
            contains:
              - "H214E/1"
              - "95 mm"
              - "120 mm"
            not_contains:
              - "error"
              - "not found"
            semantic: "Should provide complete product measurements including diameter and height specifications"
        validators:
          usage:
            "gpt-4.1-mini:output_text_tokens":
              max: 2000
```

**How this test works:**
1. **Input**: User asks for measurements of product H214E/1
2. **Steps Validation**: 
   - `contains`: Verifies `search_by_reference` tool was called with correct parameters
   - `semantic`: Uses LLM to verify search logic was appropriate
3. **Output Validation**:
   - `contains`: Checks for specific measurement values and product code
   - `not_contains`: Ensures no error messages appear
   - `semantic`: Validates that response provides comprehensive product information
4. **Usage Validation**: Monitors token consumption within specified limits

### Example 2: Multi-turn Conversation with Memory

```yaml
- name: eval_memory_retention
  description: Test agent's ability to remember information across turns
  turns:
    - input: "Hi, my name is David and I work in engineering"
      output: "Nice to meet you David! How can I help you?"
    - input:
        prompt: "What is my name and what is my profession?"
      steps:
        validators:
          not_contains:
            - name: search_external
          semantic: "Must not contain any tools"
      output:
        content:
          validators:
            contains: ["David", "engineering"]
            semantic: "Should recall both name and profession from previous turn"
```

**How multi-turn tests work:**
- **Turn 1**: Establishes context (input and output only) by providing information to remember - no validators needed
- **Turn 2**: Tests memory by asking for previously provided information - contains validators to verify behavior
- **Memory validation**: Ensures agent retrieves information from conversation history rather than external sources

### Example 3: File Processing with Error Handling

```yaml
- name: eval_file_processing
  description: Test file upload and processing with error scenarios
  turns:
    - input:
        prompt:
          - "Analyze this document for key metrics"
          - "@./test_data/report.pdf"
      steps:
        validators:
          contains:
            - name: process_pdf
              input:
                file_path: report.pdf
            - name: analyze_metrics
          not_contains:
            - name: delete_file
          semantic: "Should process the PDF file and then analyze it for metrics"
      output:
        content:
          validators:
            contains: ["metrics", "analysis"]
            not_contains: ["error", "failed", "unable"]
            regex: ".*\\d+.*%.*"
            semantic: "Should provide quantitative analysis with specific metrics"
```

### Example 4: Time and Usage Validation

```yaml
- name: eval_time_validation
  description: Test time validator for output and steps
  turns:
    - input:
        prompt: "Calculate 10 + 20 and get current time"
      steps:
        validators:
          contains:
            - name: calculate_expression
            - name: get_current_time
              validators:
                time:
                  max: 0.5
      output:
        content:
          validators:
            contains: ["30"]
        validators:
          time:
            max: 10.0
          usage:
            "gpt-4.1-mini:output_text_tokens":
              max: 500
```

## Understanding Eval Results

Timbal generates comprehensive evaluation results in JSON format:

```json
{
  "total_files": 1,
  "total_tests": 3,
  "total_validations": 5,
  "inputs_passed": 0,
  "inputs_failed": 0,
  "outputs_passed": 1,
  "outputs_failed": 2,
  "steps_passed": 0,
  "steps_failed": 1,
  "execution_errors": 0,
  "tests_failed": [
    {
      "test_name": "eval_product_search",
      "test_path": "evals/product.yaml::eval_product_search",
      "input": {
        "prompt": ["Find product X123"],
        "files": []
      },
      "reason": ["steps", "output"],
      "execution_error": null,
      "input_passed": null,
      "input_explanations": [],
      "output_passed": false,
      "output_explanations": [
        "Response did not include required product specifications"
      ],
      "actual_output": {
        "text": "I couldn't find that product.",
        "files": []
      },
      "expected_output": {
        "content": {
          "validators": {
            "semantic": ["Should provide product details and availability"]
          }
        }
      },
      "steps_passed": false,
      "steps_explanations": [
        "No step found with tool 'search_product_catalog'."
      ],
      "actual_steps": [
        {
          "tool": "general_search",
          "input": {"query": "X123"}
        }
      ],
      "expected_steps": {
        "contains": [
          {
            "name": "search_product_catalog"
          }
        ]
      }
    }
  ]
}
```

### Result Analysis

- **Summary metrics**: 
  - `total_tests`: Number of test cases executed. Each test validates one turn (the last turn in a multi-turn test), with previous turns providing conversation context.
  - `total_validations`: Total number of validators executed across all tests. Includes validators for input, output, steps, and step input parameters.
- **Detailed failures**: Complete information about each failed test including:
  - **Actual vs Expected**: What the agent actually did vs what was expected
  - **Explanations**: Detailed reasons for failures from each validator
  - **Execution errors**: Runtime errors during test execution
- **Usage monitoring**: Resource consumption tracking for cost and performance optimization

## Summary

Timbal's evaluation framework provides:

- **Comprehensive Testing**: Validate outputs, tool usage, and resource consumption
- **Flexible Validation**: From exact string matching to semantic LLM-powered checks  
- **Multi-turn Support**: Test complex conversational flows and memory retention
- **Detailed Reporting**: Rich failure analysis for debugging and improvement
- **CI/CD Integration**: Automated testing to prevent regressions

This evaluation system helps you build reliable, testable AI agents that consistently produce correct results and follow expected processes, giving you confidence in your agent's behavior across different scenarios and edge cases.
