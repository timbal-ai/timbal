---
title: "Overview"
description: "Automated testing framework for Timbal agents and runnables"
---

Evals are automated tests that validate your agent's behavior, outputs, and execution patterns. They help you ensure your agents perform correctly and consistently across different scenarios.

## Why Evals Matter

AI agents are non-deterministic - the same input can yield different results. Evals help you:

- **Validate outputs**: Ensure agents produce correct responses
- **Check tool usage**: Verify agents use the right tools with correct inputs
- **Monitor performance**: Track execution time and token usage
- **Catch regressions**: Prevent breaking changes during development
- **Test execution patterns**: Validate sequential and parallel tool execution

## How Evals Work

Timbal's eval system uses a YAML-based test definition format with a powerful validator system:

```yaml
- name: time_in_madrid
  description: Test that agent returns the time in Madrid
  runnable: agent.py::agent
  tags: ["datetime", "smoke"]
  timeout: 30000
  
  params:
    prompt: "what time is it in madrid"
  
  output:
    type!: "string"
    contains!: ":"
    pattern!: "\\d{1,2}:\\d{2}"
  
  elapsed:
    lt!: 6000
  
  seq!:
    - llm
    - get_datetime
    - llm
```

### Core Components

<CardGroup cols={2}>
  <Card title="Validators" icon="check" href="/evals/validators">
    20+ validators for checking outputs, patterns, types, and more
  </Card>
  <Card title="Flow Validators" icon="arrow-right" href="/evals/validators/flow">
    Validate execution sequences and parallel tool calls
  </Card>
  <Card title="LLM Validators" icon="brain" href="/evals/validators/llm">
    AI-powered semantic validation for natural language
  </Card>
  <Card title="CLI" icon="terminal" href="/evals/running-evals">
    Command-line interface for running and discovering evals
  </Card>
</CardGroup>

## Quick Start

### 1. Create a test file

Create a file named `eval_greeting.yaml`:

```yaml
- name: greeting_test
  description: Verify the agent greets users appropriately
  runnable: agent.py::my_agent
  
  params:
    prompt: "Hi there!"
  
  output:
    not_null!: true
    type!: "string"
    semantic!: "A polite greeting that acknowledges the user"
  
  elapsed:
    lt!: 5000
```

### 2. Run your evals

```bash
python -m timbal.evals path/to/eval_greeting.yaml
```

### 3. View results

The CLI displays pytest-style output with pass/fail status, duration, and detailed failure information:

```
========================= timbal evals =========================
collected 1 eval

eval_greeting.yaml
  greeting_test ......................................... PASSED (0.45s)
    tags: greeting, basic
    ├── output
    │   ├── not_null! ✓
    │   ├── type! ✓
    │   └── semantic! ✓
    └── elapsed
        └── lt! ✓

========================= 1 passed in 0.45s =========================
```

## Eval Structure

Each eval consists of:

| Field | Description | Required |
|-------|-------------|----------|
| `name` | Unique identifier for the eval | Yes |
| `runnable` | Path to the runnable (`file.py::name`) | Yes |
| `params` | Input parameters for the runnable | No |
| `description` | Human-readable description | No |
| `tags` | List of tags for filtering | No |
| `timeout` | Maximum execution time in milliseconds | No |
| `env` | Environment variables dict | No |
| `output` | Validators for the final output | No |
| `elapsed` | Validators for total execution time | No |
| `seq!` | Sequence flow validator | No |
| `parallel!` | Parallel execution validator | No |
| `<span_name>` | Validators for specific tool spans | No |

## Validators Overview

Validators check specific properties and use the `name!` suffix convention:

```yaml
output:
  type!: "string"
  min_length!: 10
  contains!: "success"
  not_contains!: "error"

get_datetime:
  input:
    timezone:
      eq!: "Europe/Madrid"
  output:
    type!: "string"
    pattern!: "^\\d{4}-\\d{2}-\\d{2}"
```

### Validator Categories

| Category | Validators | Purpose |
|----------|------------|---------|
| **Comparison** | `eq!`, `contains!`, `not_contains!`, `pattern!`, `starts_with!`, `ends_with!` | String and value matching |
| **Numeric** | `lt!`, `lte!`, `gt!`, `gte!` | Numeric and date comparisons |
| **Type** | `type!`, `json!`, `email!`, `not_null!` | Type and format validation |
| **Length** | `length!`, `min_length!`, `max_length!` | Length constraints |
| **LLM** | `semantic!`, `language!` | AI-powered validation |
| **Flow** | `seq!`, `parallel!` | Execution pattern validation |

## Target Resolution

Validators can target different parts of the execution:

```yaml
# Final output
output:
  contains!: "result"

# Overall timing (milliseconds)
elapsed:
  lt!: 5000

# Specific tool span
get_weather:
  input:
    city:
      eq!: "Madrid"
  output:
    type!: "object"
  elapsed:
    lt!: 2000

# Token usage
llm:
  usage:
    input_tokens:
      lte!: 500
```

See [Tracing](/evals/tracing) for details on available targets.

## Next Steps

<CardGroup cols={2}>
  <Card title="Writing Evals" icon="pen" href="/evals/writing-evals">
    Learn the full eval syntax and best practices
  </Card>
  <Card title="Validators Reference" icon="book" href="/evals/validators">
    Complete reference for all validators
  </Card>
  <Card title="Running Evals" icon="play" href="/evals/running-evals">
    CLI options and CI/CD integration
  </Card>
  <Card title="Tracing" icon="magnifying-glass" href="/evals/tracing">
    Understanding execution traces and targets
  </Card>
</CardGroup>
